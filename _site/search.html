<!DOCTYPE html>
<html lang=" en "><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Algorithms | CSI 5007 Notes</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Algorithms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CSI 5007 Notes" />
<meta property="og:description" content="CSI 5007 Notes" />
<link rel="canonical" href="http://localhost:4000/CSI-5007/search.html" />
<meta property="og:url" content="http://localhost:4000/CSI-5007/search.html" />
<meta property="og:site_name" content="Algorithms" />
<script type="application/ld+json">
{"description":"CSI 5007 Notes","@type":"WebPage","url":"http://localhost:4000/CSI-5007/search.html","headline":"Algorithms","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/CSI-5007/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/CSI-5007/feed.xml" title="Algorithms" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/CSI-5007/">Algorithms</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/CSI-5007/about/">About</a></div>
      </nav></div>
</header>
<form id='search' action="/CSI-5007/search.html" method="get">
        <label for="search-box">Search</label>
        <input type="text" id="search-box" name="query">
        <input type="submit" value="search">
      </form>

    <main class="page-content" aria-label="Content">
        <div class="wrapper">
            <div class="search">
	<ul id="search-results"></ul>

<script>
  window.store = {
    
      "notes-algorithms-basics": {
        "title": "Algorithms Basics",
        "author": "",
        "category": "",
        "content": "  Algorithm Design Guidelines          Properties of a Good Algorithm        Correctness          Incorrectness        Computational Problem          Problem Statement      Instance        Vocab  Greedy AlgorithmsAlgorithm Design Guidelines  Always know brute-force approach          Will give a baseline runtime analysis        Avoid computation by saving states  Simplify by pre-computing  Divide and Conquer  ScanningProperties of a Good Algorithm  Correctness  Efficiency  Easy to ImplementCorrectness  An algorithm is correct if, for every input instance, it halts with the correct output.Incorrectness  An algorithm is incorrect if there is any instance that produces a counter-example.          A counter-example is any instance which yields an incorrect answer.      Computational Problem  Algorithms are tools that solve computational problems.          Algorithms do so by describing a specific computational procedure for achieving the input/output relationship.      Problem Statement  Set of allowed input instances.  Required properties of algorithm’s output.Instance  The input needed to compute the solution to a computational problem.Vocab  Sorting in place          Only a constant number of array elements are stored outside of the input array at any time.        Stable          A sort is stable if two elements with equivalent keys maintain their relative position after the sort.      Greedy Algorithms  At a choice within an algorithm, the algorithm makes the best choice at the moment.  Greedy algorithms make the decision of what to do next by selecting the best local option from all available choices without regard to the global structure. -Algorithm Design Manual",
        "url": "/notes/algorithms-basics/"
      }
      ,
    
      "notes-binary-heap": {
        "title": "Binary Heap",
        "author": "",
        "category": "",
        "content": "  Terms  Binary Trees          Full Binary Trees      Complete Binary Trees      Binary Heap Data Structure      max-heap property      min-heap property      TermsHeight  Number of edges on the longest path from the root node to a leaf.Binary Trees  Each node can have up to two children, a left child and a right child  Each node has exactly one parent node (except for the root)Full Binary Trees  Every leaf has the same depth and every non-leaf has two children.Complete Binary Trees  At the deepest level, all the nodes are as far left as possible.  A complete binary tree of depth $n$ has a max number of leaves of depth $2^n$            Operation      What it does                  $d$      depth              $2^d$      Max Number of Leaves              $2^{d+1}-1$      Max Number of Nodes      Binary Heap Data Structure  Uses an array          Book uses an array object A      Object has two attributes                  A.length                          Gives number of elements in the array.                                A.heapsize                          Number of elements in the heap stored in array A                                            Root is stored in position 1The parent of a node at index $i$ is $\\lfloor{i/2}\\rfloor$def parent(i):\treturn math.floor(i / 2)The left child of a node at index $i$ is $2i$def leftChild(i):\treturn 2 * iThe right child of a node at index $i$ is $2i+1$def rightChild(i):\treturn (2 * i) + 1max-heap property$A[parent(i)] \\ge A[i]$  A node cannot have a greater value than its parent.  The largest element is the root.  The minimum elements are the leaves.min-heap property$A[parent(i)] \\le A[i]$  A parent node cannot have a greater value than its children.  The minimum element is the root.  The max elements are the leaves.",
        "url": "/notes/binary-heap/"
      }
      ,
    
      "notes-brilliant-notes": {
        "title": "Brilliant Notes",
        "author": "",
        "category": "",
        "content": "  Runtime Analysis          Comparisons      Swaps      General Solution        Recursion          Recursive Algorithms                  Fibonacci Sequence                    Calculate Total Recursions        Dynamic Programming          Steps      Runtime AnalysisComparisons  For any size 4 input, there will always be 6 comparisons made with two loopsfor i in range(n-1):\tfor j in range(i+1, n):\t\tif lst[i] &gt; lst[j]\t\t\tlst.switch(i, j)  Comparisons occur at if lst[i] &gt; lst[j]  Inner loop having a lower bound of 1 above the outer loop’s counter (i + 1) results in every pair within the list being compared.$\\mid a \\mid = \\binom{4}{2} = 6$Swaps  Count how many swaps occur per comparison.  Here, at most, one swap occurs per comparison.  The number of swaps to sort a size 4 array with the above code depends on how the original array is ordered and the resulting order.  The best way is to get an upper bound on the worst case scenario, which would occur here when sorting a reverse-sorted array.General Solution  Use pigeon-hole principleRecursionRecursive AlgorithmsFibonacci Sequencedef fibonacci(n):    if n == 1 or n == 2:        return 1        else:        return fibonacci(n-1) + fibonacci(n-2)Calculate Total Recursionsdef num_operations_fibonacci(n):    if n in [1, 2]:        return 0    else:        return 1 + num_operations_fibonacci(n-1) + num_operations_fibonacci(n-2)Dynamic Programming  Induction          Small smaller case and iterate through larger cases until the original, complex problem is solved.      Steps  Sort the data (if necessary) so that it lends itself to a dynamic approach.  Solve for the simplest case (Often $n=0$ and/or $n=1$).  Recognize patterns for small  to see how they depend on smaller $n$.  Come up with a recursive way to derive all values based on values that come before them.",
        "url": "/notes/brilliant-notes/"
      }
      ,
    
      "notes-class-code": {
        "title": "Class Code",
        "author": "",
        "category": "",
        "content": "All python code that appears in the slides.  Bubble Sort  Insertion Sort  Linear Search  Binary Search          Iterative      Recursive        Find in Log M          Python Interpretation        Contiguous Subarray Sum          Brute Force      Pre-Computing      Recursive      Scanning                  Return Indices                      Matrix Multiplication          Strassen’s Algorithm        Merge Sort  Shell Sort  Abstract (Selection) Sort  Min-Heap Heapsort  Quicksort      - Runtime          Faster Quicksort        Bin Sort  Radix Sort  Min and Max  Dutch Flag  Squaring without Multiplication          Iterative      Recursive      Iterative        Unweighted Graph Algorithms          Breadth First Search      Depth First Search (DFS)                  Iterative                          Pre-order              Post-order                                          Recursive      Graph Data Structure      Topsort        Weighted Graph Algorithms          Union-Find                  Better Implementation          Dijkstra’s Algorithm                    Bellman-Ford      Floyd-Warshall      Bubble Sort  1 - Introductiondef bs(a):    n = len(a)    for i in range(n-1):        for j in range(i+1, n):            if a[i] &gt; a[j]:                a[i], a[j] = a[j], a[i]    return a  2 - Informal Correctnessdef bs0(a):    s, n = True, len(a)    while s:        s = False        for i in range(n-1, 0, -1):            if a[i] &lt; a[i -1]:                a[i], a[i-1] = a[i-1], a[i]                s = True    return aInsertion Sort  3 - Runtime Computationsdef isort(a):    for i in range(1, len(a)):        currentvalue = a[i]        position = i        while position &gt; 0 and a[position - 1] &gt; currentvalue:            a[position] = a[position - 1]            position = position - 1        a[position] = currentvalue    return adef insertion_sort(array):    for slot in range(1, len(array)):         value = array[slot]        test_slot = slot - 1        while test_slot &gt; -1 and array[test_slot] &gt; value:            array[test_slot + 1] = array[test_slot]            test_slot = test_slot - 1        array[test_slot + 1] = value    return arrayLinear Search  5 - Divide and Conquer    def s(a, e):  for i in range(len(a)):      if a[i] == e:          return i  return -1      Binary Search  5 - Divide and ConquerIterativedef bsearch(a, e):    l, h = 0, len(a) - 1    while l &lt;= h:        m = (l+h)//2        if a[m] == e:            return m        if a[m] &lt; e:            l = m+1        else:            h = m-1    return -1Recursivedef rbsearch(a, e, l, h):    if l &gt; h:        return -1    else:        m = (l+h)//2        if a[m] == e:            return m        if a[m] &lt; e:            return rbsearch(a, e, m+1, h)        else:            return rbsearch(a, e, l, m-1)Find in Log MI don’t understand lisp so it’s only partially translated to python.( defun find−in−log−m ( v )    ( let∗ ( (max (1− ( length v ) ) )        ( symbol−m ( a ref v max) ) )    ( labels ( ( bound−in−log−m ( i )        ( if ( eql symbol−m ( aref v (− max i ) ) )            ( bound−in−log−m (∗ 2 i ) )            i ) ) )( let ( ( min ( bound−in−log−m 1 ) ) )    (1+ ( nth−value 1 ( com.informatimago.common−lisp( lambda (index)( cond    ( ( eql symbol−m ( aref v index    ( ( eql symbol−m ( aref v (1+    ( t +1)))    min max ) ) ) ) ) ) )Python Interpretationdef findInLogM(v):        n = len(v) - 1        symbol_m = v[n]              def boundInLogM(i):                  if (symbol_m == v[n-i]:                        boundInLogM(i*2)                else:                        return iContiguous Subarray Sum  6 - RecapBrute Forcedef A0(a):    n, largest = len(a), 0    for i in range(n):        for j in range(i, n):            s = sum(a[i:j+1])            largest = max(largest, s)    return largestRuntime: $\\Theta(n^3)$def A1(a):    n, largest = len(a), 0    for i in range(n):        s = 0        for j in range(i, n):            s += a[j]            largest = max(largest, s)    return largestRuntime: $\\Theta(n^2)$Pre-Computingdef A2(a):    c = [0] * (len(a) + 1)    for i in range(len(a)):        c[i+1] = c[i]+a[i]    largest = 0    for i in range(len(a)):        for j in range(i, len(a)):            s = c[j+1]-c[i]            largest = max(s, largest)    return largestRuntime: $\\Theta(n^2)$Recursivedef A3(a):    n = len(a)    if n == 0:        return 0    if n == 1:        return max(0, a[0])    m = n//2    MA, MB, s, MCl = A3(A[0:m]), A3(a[m:n]), 0, 0    for i in range(m, 0, -1):        s = s + a[i]        MCl = max(s, MCl)    s, MCr = 0, 0    for i in range(m+1, n):        s = s + a[i]        MCr = max(MCr, s)    return max(MA, MB MCl + MCr)Runtime: $\\Theta(nlog_2(n))$Scanningdef A4(a):    mf = 0    mh = 0    n = len(a)    for i in range(n):        mh = max(mh+a[i], 0)        mf = max(mf, mh)    return mfRuntime: $\\Theta(n)$Return Indicesdef A4ix(a):    mf = 0    mh = 0    n = len(a)    s = 0    e = -1    sh = 0    eh = -1    for i in range(n):        if mh+a[i] &gt; 0:            eh = i            mh += a[i]        else:            mh = 0            sh = i + 1        if mh &gt; mf:            mf = mh            s = sh            e = eh    return mf, s, eMatrix Multiplication  8 - Matrix Multiplicationdef size(A):    return len(A), len(A[0])def mm(A, B):    (p, q) = size(A)    (q, r) = size(B)    C = [[0 for _ in range(r)] for _ in range(p)]    for i in range(p):        for j in range(q):            for k in range(r):                C[i][k] += A[i][j] * B[j][k]    return CRuntime: $\\Theta(pqr)$$\\Theta(n^3)$ if all matrices are size $n$Strassen’s Algorithmdef strassen(A, B, n, n0):    if n &lt;= n0:        return mm(A,B)    else:        m = n // 2        u = range(m)        v = range(m+1, n)        P1 = strassen(A(u, u) + A(u, v), B(u, u) + B(u, v), m, n0)        P2 = strassen(A(v, u) + A(v, v), B(u, u), m, n0)        P3 = strassen()        #Missing P4, P5, P6, P7        #P4 = A22(B21 - B11)        #P5 = B22(A11 + A12)        #P6 = (A21 - A11)(B11 + B12)        #P7 = (A12 - A22)(B21 + B22)        C(u, u) = P1 + P4 - P5 + P7        C(u, v) = P3 + P5        C(v, u) = P2 + P4        C(v, v) = P1 + P3 - P2 + P6    return CRuntime: $\\Theta(n^{2.8})$Merge Sort  9 - The Heap and the Quickdef merge_sort(array):    if len(array) &lt;= 1:        return array    else:        m = floor(len(array) / 2)   return merge(array[0:m], merge_sort(array[m:]))def merge(a, b):    ia = 0    ib = 0    ic = 0    na = len(a)    nb = len(b)    nc = na + nb    c = [0] * nc    while(ic &lt; nc):        if(ia &lt; na):            if(ib &lt; nb):                if(a[ia] &lt; b[ib]):                    c[ic] = a[ia]                    ic += 1                    ia += 1                else:                    c[ic] = b[ib]                    ic += 1                    ib += 1            else:                c[ic] = a[ia]                ic += 1                ia += 1        else:            c[ic] = b[ib]            ic += 1            ib += 1    return cRuntime: $\\Theta(nlog(n))$Shell Sort  9 - The Heap and the Quickdef shellSort(gaps):    for gap in gaps:        for i in range(gap):            gaplSort(lst, i, gap)def gaplSort(lst, start, gap):    for i in range(start + gap, len(lst), gap):        v = lst[i]        p = i        while p &gt;= gap &amp; lst[p-gap] &gt; v:            lst[p] = lst[p - gap]            p = p - gap        lst[p] = vRuntime: $O(n^{3/2})$ to $O(nlog^2(n))$Abstract (Selection) Sort  9 - The Heap and the Quickdef abstractSort(a):    n = len(a)    na = []    for i in range(n):        smallest = extractSmallestAndDelete(a)        na.append(smallest)    return naMin-Heap Heapsort  9 - The Heap and the Quickdef newheap(n):    return [0]*(n+1)def insert(a, e):    a[0] = a[0] + 1    a[a[0]] = e    heapfixup(a,a[0])def extractsmallest(a):    e = a[1]    a[1] = a[a[0]]    a[0] = a[0] - 1    heapfixdown(a, 1)    return edef heapfixup(a,i):    while i &gt; 1:        p = i // 2        if a[p] &gt; a[i]:            a[p], a[i] = a[i], a[p]            i = p        else:            returnRuntime: $\\Theta(log(n))$def heapfixdown(a, i):    while (2 * i) &lt;= a[0]:        c = 2 * i        if (c + 1) &lt;= a[0]:            if a[c + 1] &lt; a[c]:                c += 1            if a[i] &gt; a[c]:                a[i], a[c] = a[c], a[i]                i = c            else:                returnRuntime: $\\Theta(log(n))$def heapsort(x):    n = len(x)    a = newheap(n)    for i in range(n):        insert(a, x[i])    for i in range(n):        x[i] = extractsmallest(a)    return xRuntime: $\\Theta(nlog(n))$Quicksort  9 - The Heap and the Quickdef partition(a, l, u):    t = a[l]    m = l    for i in range(l+1, u+1):        if a[i] &lt; t:            m = m + 1            a[i] = a[m]            a[m] = a[i]        a[m] = a[l]        a[l] = a[m]    return mdef qsort0(a, l=0, u=None):    if u is None:        u = len(a) - 1    if l &lt; u:        m = partition(a, l, u)        qsort0(a, l, m-1)        qsort0(a, m+1, u)    return aRuntimeBest Case: $O(nlogn)$Worst Case: $O(n^2)$Average Case: $O(nlogn)$Faster Quicksortdef qsort1(a, l=0, u=None):    if u is None:        u = len(a) - 1    if l &lt; u:        m = partition(a, l, u)        qsort0(a, l, m-1)        qsort0(a, m+1, u)    elif l &lt; u:        insertion_sort(a, l, u)    return aBin Sort  10 - Limits of Sortingdef binsort(a):    bins = [a]    for l in range(len(a[0]) - 1, -1, -1):        binsTwo = [[] for _ in range(10)]        for bin in bins:            for e in bin:                binsTwo[e[l]].append(e)        bins = binTwo    return [e for bin in bins for e in bin]Running Time:$\\Theta(n)$Radix Sort  10 - Limits of Sortingdef radixSort(a, d):    for i in range(d):        sortStably(a, lambda a0, a1 : a0[i] &lt; a1[i])Min and Max  10 - Limits of Sortingdef minandmax(a):    min = max = a [0]    for i in range(1, len(a)):        if a[i] &lt; min:            min = a[i]        elif a[i] &gt; max:            max = a[i]    return min, maxNumber of comparisons is between $n$ and $2n$def minandmax2(a):    min, max = a[0], a[1] if a[0] &lt; a[1] else a[1], a[0]    for i in range(2, len(a), 2):        j = i + 1        if a[i] &lt; a[j]:            if a[i] &lt; min:                min = a[i]            if a[j] &gt; max:                max = a[j]        else:            if a[j] &lt; min:                min = a[j]            if a[i] &gt; max:                max = a[i]    return min, maxNumber of comparisons is $3n/2$Dutch Flagdef dutch_flag(a, red = 0, white = 1, blue = 2):    r = 0    w = 0    b = len(a) - 1    n = len(a) - 1    while w &lt;= b:        if a[w] == red:            a[r], a[w] = a[w], a[r]            r += 1            w += 1        elif a[w] == blue:            a[b], a[w] = a[w], a[b]            b -= 1        elif a[w] == white:            w += 1    return aSquaring without MultiplicationIterativedef sq_no_mult_0(n):    n2 = 0    for i in range(n):        n2 += n    return n2Recursivedef sq_no_mult_r(n, count = -1, n2 = 0):    if count == 0:        return n2    else:        if count == -1:            count = n        return sq_no_mult_r(n, count-1, n2 + n)Iterativedef sq_no_mult_1(n):    #Assumes n = 2^k    n2 = n    n1 = 1    while n1 &lt; n:        n1 += n1        n2 += n2    return n2def sq_no_mult_2(n):    n2 = n    n1 = 1    while n1 + n1 &lt; n:        n1 += n1        n2 += n2    m = n - n1    n3 = n1 + m    n4 = 0    for i in range(m):        n4 += n3    return n2 + n4Unweighted Graph AlgorithmsBreadth First Searchdef bfs(Graph, node):    enqueue(node)    visited[r] = True    while v = deque():        for u in neighbors(Graph, v):            if not visited[u]:                enqueue(u)                visited[u] = True  queue          Last-In First-Out (LIFO) data structure        enqueue          Adds an element to the queue        dequeue          Extract oldest element of the queue.      def components (G ):    count = 0    for v in vertices (G):    if not visited [v] :        count = count + 1        BFS(G, v)    return countdef bipartite (G) :for v in vertices (G) :    if not visited [v] :        colour [v] = red        BFS(G, v)def process (x,y) :    if colour [x] == colour [y]:        abort (\"Not␣bipartite\")    else:        colour[y] = complement (colour[x])def bfs(Graph, node, process = None):    enqueue(r)    visited[r] = True    while v = dequeue():        for u in neighbors(Graph, v):            if process = True:                process(v, u)            if not visited[u]:                enqueue(u)                visited[u] = TrueDepth First Search (DFS)IterativePre-orderdef dfs(Graph, node):    push(r)    while u = pop():        visited[u] = True        for v in neighbor(Graph, u):            if not visited[v]:                push(v)Post-orderdef dfs(Graph, node):    push(r)    while u = pop():        for v in neighbor(Graph, u):            if not visited[v]:                push(v)        visited[u] = TrueRecursivedef dfs(Graph, node):    visited[r] = True    for v in neighbor(Graph, node):        if not visited[v]:            dfs(Graph, v)Graph Data Structuredef neighbors(Graph, v):    return Graph[v]def nodes(Graph):    return list(Graph)Topsortdef topsort(Graph):    # initialize empty stack    stack = []    visited = [0] * (len(nodes(Graph)) + 1)    for v in nodes(Graph):        if visited [v] == 0:            if visit(Graph, v, stack, visited) == False:                return None    return stackdef visit(Graph, node, stack, visited):    if visited[node] == 2:        return True    elif visited[node] == 1:        return False    visited[node] = 1    for v in neighbors(Graph, node):        if visit(Graph, v, stack, visited) == False:            return False        stack.insert(0, node)        visited[r] = 2        return TrueWeighted Graph AlgorithmsUnion-Finddef initialize(n):    global p    p = [None] * ndef find(u):    global p    if p[u] == None:        return u    else:        return find(p[u])def union(u,v):    global p    pu, pv = find(u), find(v)    p[pu] = pvBetter Implementationdef initialize(n):    global p, r    p, r = [None]*n, [0]*ndef find(u):    global p    return u if p[u] == None else find(p[u])def union(u,v):    global p, r    pu, pv = find(u), find(v)    if r[pu] &lt; r[pv]:        p[pu] = pv    elif r[pv] &lt; r[pu]:        p[pv] = pu    else:        p[pv], r[pu] = pu, r[pu]+1Runtime: $\\Theta(logn)$Dijkstra’s Algorithmdef dijkstra(Graph, node):    Tree, d = {node}, [X] * size(Graph)    d[node] = 0    for v in neighbors(Graph, node):        d[v] = weight(Graph, (node, v))    while size(Tree) &lt; size(Graph):        v = Cheapest(Tree, d)        Tree.append(v)        for u in neighbors(Graph, v):            if d[u] &gt; d[v]+weight(Graph, (v,u)):                d[u] = d[v]+weight(Graph, (v,u))Runtime: $O(V^2)$Using Minheap: $O(E+V)log(V)$Fibonacci heap: $O(E + VlogV)$Bellman-Forddef bellman_ford(Graph, node):    d = [\"infinity\"] * size(Graph)    d[node] = 0    for _ in range(size(Graph) - 1):        for (u, v) in edges(Graph):            if d[v] &gt; d[u] + weight(Graph, (u,v)):                d[v] = d[u] + weight(Graph, (u,v)):                    p[v] = u    return p, dFloyd-Warshalldef floyd_warshall(Graph):    a = [[w(u,v) if (u,v) in edges(Graph) else \"infinity\"] for u in nodes(Graph)] for v in nodes(Graph)]    for k in nodes(Graph):        for u in nodes(Graph):            for v in nodes(Graph):                a[u][v] = min_k(a[u][v], a[u][k] + a[k][v])    return aRuntime: $\\Theta(V^3)$",
        "url": "/notes/class-code/"
      }
      ,
    
      "notes-class-notes-june-18-june-18": {
        "title": "June 18",
        "author": "",
        "category": "",
        "content": "$x_{1,j}=1$ - Demand$x_{i,n}=1$ - SupplyTwo families of methods: simplex methods or interior-point methodsSimplex Methods“Standard Form”  Decide to always maximizeTo convert an equality into an inequality:Then convert $ax_1+bx+2\\ge 5$ to $-ax_1-bx_2\\le -5$P vs NPRuntime  Before $n$ was defined loosely as the size of the input (number of elements in array)  An algorithm running in polytime, $n$ is the number of bits of input.  Need to be polynomial in size of the input.Example  Knapsack Problem  Runtime: $O(nW)$  Size of the input:          In bits: $O(128\\bullet n\\bullet logW)$                  The size of the input = $128\\bullet n$          The number of bits for an integer is $log(integer)$                    Deterministic Turing Machine  A Turing machine is a tape with R/W head.          Reads from the tape then moves a step ahead and writes.        “Deterministic”          There are no parallel threads.      The class $P$  A problem P is in the class $P$ if there is an algorithm running in time $P$  Sorting  Searching  Matrix MultiplicationThe class $NP$  Does not mean “not polynomial”  If you take an instance of some problem and some additional data, you can answer Yes or No.          “Is there __ that would equal __”      Must provide data that would equal that condition so it can be answered as Yes or No        Problem:          Problem to be solved.        Certificate:$P\\subset NP$  Proves “not polynomial” as that would read “polynomial is a subset of not polynomial”$P\\subsetall NP$  To disprove find one problem that is P and prove that it is not in NP          Proving that it is not in NP is not possible to show.      $P=NP$  Need to show that all problems in $P$ are in $NP$The class Co-$NP$  Give me a problem, give me an instance, you can answer “No in polytime”Problems in Co-$NP\\cap NP$  $\\all P$Every problem instance with this type of certificate.ExamplesSAT Problem  Given a Boolean is there a true solution?  Certificate to verify Yes: the solution itself.          The certificate is a solution.        The problem class of SAT is therefore in $NP$Tautology  Given a boolean is it always true?  You cannot give a certificate to verify yes.  You can give a certificate to verify no.  This problem class is in Co-$NP$  You don’t care about solving the problem, you are verifying a solution that is given.          If you can verify the solution in polytime, then it is in $NP$      Reducability  A problem instance can be reduced to another problem instance.  If it was possible to reduce the original problem instance to another problem instance, the answer can be extracted to the original problem instance.The Class $NP$-Complete  A problem $P$ is in class $NP$-complete if it is in $NP$ and if all other problems in $NP$ can be reduced to $P$",
        "url": "/notes/class_notes/june-18/june-18/"
      }
      ,
    
      "notes-class-notes-june-20": {
        "title": "June 20",
        "author": "",
        "category": "",
        "content": "$m(u)$ - Optimal number of containers for $u$  $u$ is a vector of size $u$  $u_1$ is the number of elements of $w_1$, etc.The optimal solution is 1 if it fits in the container      If the input is 0, you have 0 containers    Each $V$ fits in a container          $min$ means optimal solution      $min(m)$ is the minimum number of bins over all configurations.        If it doesn’t fit in a container,  $a\\in{V}$ is every possible configuration you can extract.",
        "url": "/notes/class_notes/june-20/"
      }
      ,
    
      "notes-class-notes-june-11": {
        "title": "June_11",
        "author": "",
        "category": "",
        "content": "Must Know AlgorithmsPrim, Kruskal, DijkstraHashmap  No orderingObjectives  Any data types.  Average Runtime          $O(1)$        Array is fixed length with fixed element size.Assumptions  A universe of $U$ keys  Map keys to ${1, 2, …, \\lvert U\\rvert }$  A string has a mapping to an integer so that each character has an integer.          Allows converting from string to integer.      Hash Function  Input: a big number${1,2,…,\\lvert U\\rvert }$  Output: An integer which is a position in the array.${1,…,n-1}$Collision  Two keys that map to same place  Unavoidable if universe of keys is larger than array.          Pigeonhole principle        Resolved by chaining          An index that contains two objects will be a linked list.      First element is head of list and points to other pointers with same hashmap key.                  Affects runtime as must iterate through linked list.                      Chained-Insert(H,x)          Insert at head of linked list      $O(1)$        Chained-Search(H,k)          Search for element with key k      $O(l)$                  $l$ is list length                    Runtime  Based on load factor$\\Theta(1+\\alpha)$ on averageExample  If you have 10x the elements of total positions in the array, you would have 10 items in each slot.          This only gives $\\alpha = 10$ so it is close to $\\Theta(1)$      Load Factor$\\alpha = \\frac{n}{m}$  Could be as small as 0      No upper bound    Keep the load factor low to keep runtime $O(1)$          Reallocate to a new array to keep the number of slots high.      Hash FunctionsCollisionDivision Method$h(k)=k mod m$  Don’t want m to be power of 2  Needs to use all the bits of the key          Use primes      Multiplicative Method  Faster than division          Pick a fraction between 0 and 1$h(k)=\\lfloor m(kA-\\lfloor kA\\rfloor)\\rfloor$        Want m to be a power of 2.          Multiplication by 2 is a shift.      Store $kA$ as it is calculated twice.      Open Addressing  No linked list  All elements stored in array of length $m$  Never let load factor get to 1  Uses a probe sequence to find empty slotsProbingLinear Probing$h(k,i)=h’(k)+i mod m$  $h’(k)$ is one of the hash functions.  Clustering occurs as it continually checks the next available slot.          If a slot is filled it will choose the next.      Quadratic Probing  No clusteringDouble Hashing  Use one that is a multiplication hash function and one that is division.          $h’‘(k)$ is either multiplication or division      $h’(k)$ is the other      Negatives of Open Addressing  When reallocating, you must rehash every item because the hash functions depend on the size of the array, $m$Dynamic ProgrammingH.get(str([p,n]), None)      Look up a string in a hashmap.  If it does not exist, return None        Bottom-up          A3 code example      Really Hard ProblemsTravelling Salesman  All solutions are cycles.  Multiple permutations may be the same cycle.          $(0,1,2)\\equiv (2,0,1)$      These are only shifted by 1 position.        Search Space              $n$ Permutations $=n!$  Find all essential tours.def calc_cost(p, D):  p is a permutation  D is distance matrix          Throughout all TSP implementations in this course, it is using a matrix.      If the existing graph is not complete, it is “made complete” by setting each nonexisting edge to $\\infty$      Tree Walkingdef visit_0(v,n)  k - current length  v - starts empty          A vector of permutation.      Runtime  Empty array          Complement is every other array.                  Call      Runtime                  []      1              [0],[1],...[n-1]      $n$              [2]-&gt;[2,0],[2,1],...[2,n-1]      $n-1$      Runtime: $O(n!)$Change Code for More efficiencyif you're at a leaf nodecompute the cost and compare to your current best.  Same permutations          If they are shifted or backwards      Shifting solved by setting the starting node to [0]                  Improves runtime by factor of $n$          Now runtime is $(n-1)!$                    Backwards may be solved by a comparison                  Improves runtime by halving                      Do minimum amount of work          Assumption: You have current best.                  If you know your minimum best, then approx. the future best and avoid that branch.          achieved by if cost &lt; best            Extension Bound                                Calculates best possible extension bound.$\\sum_{i=0}^{i=l-1}c[v_i, v_{i+1}$_markdown error",
        "url": "/notes/class_notes/june_11/"
      }
      ,
    
      "notes-dynamic-programming": {
        "title": "Dynamic Programming",
        "author": "",
        "category": "",
        "content": "  Why Dynamic Programming?  Elements of Dynamic Programming  Developing a Dynamic Program          Recursion vs. a Dynamic Program      Optimal Substructure                  Finding the Optimal Substructure                      Running Time          Subproblem Graph      “Bottom-Up”        Memoized DP Algorithm          Runtime      Fibonacci                  Runtime Analysis          Pseudocode                    Why Dynamic Programming?  Greedy algorithms make best local decision but typically not optimal.  Exhaustive algorithms produce optimal result but at the cost of efficiency.  Dynamic programming:          Guarantees correctness by systematically searching all possibilities.      Provides efficiency by storing results to avoid recomputing.      Elements of Dynamic Programming  For dynamic programming to apply, an optimization problem must have:          Optimal substructure      Overlapping subproblems        Recursion  Storing partial resultsDeveloping a Dynamic Program  Find a recursive definition to a problem.          The recursive algorithm must solve the same subproblems over and over again.                  If it does, store the answer for each and return it to the next recursive call.                      Show that the number of different parameter values taken on by your recurrence is bounded by a small polynomial.  Specify the order of evaluation for the recurrence so the partial results you need are always available when you need them.Recursion vs. a Dynamic Program  Dynamic programming uses recursion but they are not synonymous because in other recursive calls, storing values of subproblems are useless.Optimal Substructure  Characterizing the structure of an optimal solution.  An optimal solution is built from optimal solutions to subproblems.  Optimal Substructure          An optimal solution of a problem contains within it optimal solutions to subproblems.      Finding the Optimal Substructure  Keep the space as simple as possible and then expand it as necessary.          Show that a solution to a problem consists of making a choice.  This choice leaves one or more subproblems to be solved.      Suppose for the given problem, you are given the choice that leads to the optimal solution.              Given the choice, determine which subproblems ensue and how to best characterize the subproblems.      Show that solutions to the subproblems are themselves optimal.                  Suppose that each of the subproblem solutions are not optimal and then derive a contradiction.                    Running Time  Depends on the product of the number of subproblems overall and how many choices we look at for each subproblemSubproblem Graph  An alternative way of analysis  Each vertex corresponds to a subproblem.  Choices are edges incident from that subproblem.“Bottom-Up”  Optimal solutions to subproblems are found first.  Involves making a choice among subproblems as to which we will use in solving the problem.  The cost of the problem is the cost of the subproblem plus the cost of the choice itself.Memoized DP AlgorithmRecursion + memoization  Remember solutions and reuse solutions to subproblems that help solve the problem.          “Memo” comes from the idea of using a scratch pad or memo to log results (as you would in calculating an equation without memory) and using those answers to find the final answer.        Runtime              time = # of subproblems * the amount of time you spend per subproblemFibonaccimemo = {}fib(n):    if n in memo:        return memo[n]    if n &lt;= 2:        f = 1    else:        f = fib(n-1)+fib(n-2)        memo[n]=f    return f  Create empty memo dictionary  Check if fib(n) is already in the dictionary (it has already been computed)          If not, comput the fib number        Store fib(n) into memo[n] and return fRuntime Analysis  Only making recursive calls the first time fib(k) it’s called.          The memoized calls are constant time ($\\Theta(1)$)        The number of non-memoized calls is $n$ since fib(k) is done the first time and only done once for each number $&lt;= n$Runtime = $\\Theta(n)$Pseudocode  Uses a hash table for look up instead of function callsfib = {}for k in range(1, n+1):    if k &lt;= 2:        f = 1    else:        f = fib[k-1] + fib[k-2]    fib[k] = freturn fib[n]  Answers to subproblems are stored in a dictionary of $1…n$  The only answer cared about is fib[n] which is returned.",
        "url": "/notes/dynamic-programming/"
      }
      ,
    
      "notes-final": {
        "title": "Final",
        "author": "",
        "category": "",
        "content": "  TODO  Greedy Algorithms          Minimum Spanning Trees        Shortest Path  Divide and Conquer  Sorting ProblemTODO  Understand heapsort algorithms          Heapify      Heapfix        Chaining vs. Open Addressing  Adjacency Matrix ImplementationGreedy AlgorithmsMinimum Spanning Trees  Prim  Kruskal  Floyd-Warshall  Bellman-FordShortest Path  DijkstraDivide and Conquer  Merge Sort  Quicksort  Binary Search  HeapsortSorting Problem",
        "url": "/notes/final/"
      }
      ,
    
      "notes-graph-algorithms": {
        "title": "Graph Algorithms",
        "author": "",
        "category": "",
        "content": "  Basics          Runtime Analysis      Principle of Optimality        Graph Representations          Adjacency Matrix      Adjacency List                  Adjacency List Implementation                      Graph Traversal          Key Ideas                  Vertex States                      Breadth First Search          Application      Properties      Steps      Psuedocode      Runtime        Depth First Search (DFS)  Applications          Properties      Stack      Predecessor Graph      Steps      DFS Strategies                  Pre-order          In-order INCOMPLETE          Post-Order                    Pseudocode      Python Implementation                  Iterative          Recursive                    Runtime        Minimum Spanning Trees (MST)          Prim’s Algorithm                  Pseudocode                    Kruskal’s Algorithm        Dijkstra’s Algorithm  Floyd-Warshall AlgorithmBasics  graph $G=(V,E)$ contains $n$ vertices and $m$ edges          Vertices are also called nodes        $(u,v)$ is an edge.  Indicates there is an edge from vertex $u$ to $v$          $(u,v)\\ne (v,u)$ in a directed graph (di-graph)      Runtime Analysis  The runtime of a graph algorithm for a given graph $G=(V,E)$ is measured in terms of the number of vertices ($\\lvert V\\rvert$) and number of edges ($\\lvert E\\rvert$)  Inside asymptotic notation, the cardinality is ommitted.          $\\Theta(\\lvert V\\rvert, \\lvert E\\rvert)\\equiv \\Theta(V, E)$      Principle of Optimality  If $v_1,…,v_j,…,v_k$ is a shortest path from $v_1$ to $v_k$ passing through $v_j$, then the subpath $v_1,…,v_j$ is a shortest path from $v_1$ to $v_j$Graph Representations            Graph      Data Structure                  Sparse Graph $(\\lvert E\\rvert « {\\lvert V\\rvert}^2)$      Adjacency List              Dense Graph $(\\lvert E\\rvert « {\\lvert V\\rvert}^2)$      Adjancency Matrix      Adjacency Matrix  $G$ is represented by a $n \\times n$ matrix $M$  $n$ are the number of vertices in the graph.  A 1 represents an edge between vertex $i$ and $j$          adj[i][j] = w                  Weighted graphs represented in an adjacency matrix displays the weight instead of 1 for an edge.                      Pros          Rapid search for edges in graph      Rapid update to edge insertion and deletion        Cons          Uses excessive space for graphs with many vertices and few edges      Adjacency List  Pros          More efficiently stores sparse graphs        Cons          Slower to find edge as it is necessary to search from head of linked list to find edge.      Adjacency List Implementation#define MAXV 1000typedef struct {    int y;    int weight;    struct edgenode *next;} edgenode;typedef struct {    edgenode *edges[MAXV+1];    int degree[MAXV+1];    int nvertices;    int nedges;    bool directed;} graphGraph TraversalKey Ideas  Mark every vertex which is visted to keep track of if the graph has been completely explored.          There are numerous implementations of this, such as tracking visited nodes by adding to an array, or changing the state on the property of a vertex if it is coded as an object.      Vertex States            State      Description                  undiscovered      Vertex is in its initial state and has not been visited.              discovered      Vertex has been found but incident edges have not been checked.              processed      Vertex where all incident edges have been visited.      Breadth First SearchApplication  Finding optimal solution out of available options.          Find connected components        Find shortest path in unweighted graphs.  Test for bipartite property.Properties  Builds a breadth-first tree with root $s$          $s$ - A distinguished source vertex.        Explores edges of $G$ to discover every vertex that is reachable from $s$.          Vertex is discovered the first time it is encountered in a search.        Distance ($d$) is the shortest path from any vertex $v$ to the source vertex $s$  Algorithm discovers all vertices at distance $k$ from $s$ before discovering any vertices at $d = k + 1$  $u.\\pi$ represents the predecessor of $u$ in book psuedocode.  Uses a queue          Stores discvored by not processed vertices in FIFO order.                  Explore the oldest unexplored vertices first.            Steps                          Pick source vertex, s, from graph.                                  s becomes the root of the tree.                                            Look at each neighbor in an order.              Visit each neighbor vertex in that same order.              Repeat breadth-first search for each k+1 vertex from s                                          PsuedocodeBFS(G, s)    for each vertex $u\\in V[G] - {s}$ do        state[u] = \"undiscovered\"        p[u] = nil //no parent is in BFS tree    state[s] = \"discovered\"    p[s] = nil    Q = {s}    while Q != $\\emptyset$ do        u = dequeue[Q]        process vertex u as desired        for each $v \\in Adj[u]$ do            process edge $(u,v)$ as desired            if state[v] = \"undiscovered\" then                state[v] = discovered\"                p[v] = u                enqueue[Q, v]        state[u] = \"processed\"  p[*] represents predecessor of vertex  A vertex is discovered the first time it is visited.  A vertex is processed after all outgoing edges from it have been traversed.Runtime$O(n + m)$Depth First Search (DFS)Applications  Connnected components  Topological Sorting          Topological Sort                  Linear ordering of its vertices such that for every directed edge $uv$ from vertex $u$ to vertex $v$, $u$ comes before $v$ in the ordering.                      Finding cycles in directed graphsProperties  Finds the longest path of a graph $G$  Explores deep into a graph whenever possible.  Uses a stack to keep track of vertices.  Explores edges out of the most recently discovered vertex $v$ that still has unexplored edges leaving it.  Once $v$’s edges have been explored, the search backtracks to explore edges leaving the vertex from which $v$ was discovered.          Backs up when surrounded by previously discovered vertices.        Continues until all vertices $v$ that are reachable from the original source vertex $s$ have been discovered.  Algorithm repeats process until it has discovered very vertex.Stack  Stores discovered but not processed vertices.  LIFO  Predecessor Graph  Whenever a DFS discovers a vertex $v$ during a scan of an already discovered vertex $u$, it sets $v$’s predecessor attribute $v.\\pi$ to $u$  The predecessor subgraph produced by a DFS forms a predecessor forest because the search may repeat from multiple source ($s$) verticesPredecessor Subgraph RepresentationSteps  Visit a vertex $s$  Mark $s$ as visited  Recursively visit each unvisited vertex attached to $s$DFS StrategiesPre-order  Visit source node $s$.  Successively move left to visit each node ($v$) until a leaf is reached.  When there are no more children to left of $s$ node.In-order INCOMPLETE  Find left-most node in tree and visit.  Vist the parent of that node.  Visit right child of this parent node.          Find next left-most node.      Post-Order  Visit left-most leaf in tree.  Visit parent and then find second left-most leaf.  Repeat until the parent is the last node within a branch.PseudocodeInitialize an empty stack for storage of nodes, S.For each vertex u, define u.visited to be false.Push the root (first node to be visited) onto S.While S is not empty:    Pop the first element in S, u.    If u.visited = false, then:        U.visited = true        for each unvisited neighbor w of u:            Push w into S.End process when all nodes have been visited.Python ImplementationIterativedef depth_first_search(graph):    visited, stack = set(), [root]    while stack:        vertex = stack.pop()        if vertex not in visited:            visited.add(vertex)            stack.extend(graph[vertex] - visited)    return visitedRecursivedef depth_first_search_recursive(graph, start, visited=None):    if visited is None:        visited = set()    visited.add(start)    for next in graph[start] - visited:        depth_first_search_recursive(graph, next, visited)    return visitedRuntimeIf using a adjacency list, $O(V+E)$Minimum Spanning Trees (MST)  Spanning Tree          An acyclic subset, or tree, $T$ of a graph that connects all vertices from a vertex $u$ to $v$        Minimum Spanning Tree          A spanning tree that has the least weight.      Prim’s Algorithm  Starts from one vertex and grows the rest of the tree one edge at a time until all vertices are included.  Greedy          Repeatedly selects the smallest weight edge at a vertex.        Start from a given vertex $s$          This is root of a spanning tree $T$        With each iteration add a new vertex to the spanning tree $T$.          Always add the lowest-weight edge linking a vertex in the tree to a vertex outside of the tree.      PseudocodePrim-MST(G)  Select an arbitrary vertex s to start the tree from    while (there are still nontree vertices)\t  Select the edge of minimum weight between a tree and nontree vertex      Add the selected edge and vertext to the tree TKruskal’s Algorithm  More efficient than Prim’s on sparse graphs.MST-Kruskal(Graph, weight):    A = []    for each vertex (v) in Graph.vertices        MakeSet(v)    sort edges of G.E into non-decreasing order by weight        if FindSet(u) != FindSet(v):            A = A.append{(u, v)}            Union(u,v)    return ADijkstra’s Algorithm  Finds shortest path.          Does not work with negative weights      Floyd-Warshall Algorithm  Shortest path supporting negative weights.",
        "url": "/notes/graph-algorithms/"
      }
      ,
    
      "notes-hashing": {
        "title": "Hashing",
        "author": "",
        "category": "",
        "content": "  Hash Function          Actual Hash Function      Collisions                  Chaining                    Hash Function  A mathematical function that maps keys to integers.  Return value of a hash function is the index of an array of pointers.          The item associated with a key is referenced by the pointer at the index.                  Thinking of it as a pointer makes it easier later for Chaining Resolution.          If it makes it easier, think of the value array as the items themselves, or objects.                    The process:                  key (a string) -&gt; hash function -&gt; index integer -&gt; pointer located at index -&gt; object in memory the pointer pointed to                      $\\alpha$          The size of an alphabet which a string $S$ is written.        char(c)          A function that maps each symbol of the alphabet, $a$, to a unique integer from 0 to $a-1$      Actual Hash Function$H(S)=\\sum_{i=0}^{|S|-1}\\alpha^{|S|-(i+1)}\\times char(s_i)$  $m$ - Number of slots in the hash table.  Since the hash function will generate a large integer, which would require a large array to map the index accurately, the function must reduce the number to an integer between 0 and $m-1$          Use modulus to take the remainder of $H(S) mod m$ to assign and index.      The ideal table size of $m$ is a large prime not close to $2^{i}-1      Collisions  Two distinct keys that hash to the same value.Chaining",
        "url": "/notes/hashing/"
      }
      ,
    
      "notes-searching-algorithms": {
        "title": "Searching Algorithms",
        "author": "",
        "category": "",
        "content": "  Linear Search          Running Time        Binary Search          Iterative Implementation                  Explanation                          Loop Invariant                                Loop Invariant                    Recursive Implementation                  Running Time          Recurrence                    Linear Searchdef s(a, e):    for i in range(len(a)):        if a[i] == e:            return i    return -1Running Time$\\Theta(n)$Binary SearchIterative Implementation  array must be sorted.  low and high are indices not values.  key is the value being searched for.def bsearch(array, key):    low, high = 0, len(array)-1    while low &lt;= high:        middle = math.floor((low + high) / 2)        if array[middle] == key:            return middle        if array[middle] &lt; key:            low = middle + 1        else:            high = middle - 1    return -1Explanationfrom slidesAt the start the interval [l,h] encompasses the whole array and it is reduced at every step. Therefore, the algorithm terminates.Loop InvariantIf element e is in the array, it is in a[l,..,h]. The algorithm considers the mid-point and compares its element with the target e. If the element is too large, the target must be in the lower half and reduces h. If the element is too small, the element must be in the upper half and increases l. It stops when it finds e or when the array is exhausted.Loop Invariantinterpreted explanation  If the key is in the array, it is in a[1,...,high]  The midpoint is determined by dividing the array in half.          middle = math.floor((low + high) / 2)        The element array[middle] at the midpoint is compared with the key          If it matches, the index of the middle is returned.        If the key is larger than array[middle], then the key must be in the lower half.          high is set then to one less than the middle index.      high = middle - 1        If the key is smaller than array[middle], then the key must be in the upper half.          low is set then to one above the middle index.      low = middle + 1      Recursive Implementationdef binary_search(array, key, low=0, high=None):    if high is None:        high = len(array) - 1    if(low &gt; high):        return -1    middle = (low + high)// 2    if(array[middle] == key):        return middle    if(array[middle] &gt; key):        return binary_search(array, key, low, middle-1)    else:        return binary_search(array, key, middle+1, high)Running Time$\\Theta(lg(n))$Recurrence$T(n)=T(n/2)+O(1)$",
        "url": "/notes/searching-algorithms/"
      }
      ,
    
      "notes-sorting-algorithms": {
        "title": "Sorting Algorithms",
        "author": "",
        "category": "",
        "content": "  The Sorting Problem  Bubble Sort          Running Time      Loop Invariant      Python Implementation        Insertion Sort          Running Time      Python Implementation        Merge Sort          Recurrence      Running Time      Python Implementation        Selection Sort          Python Implementation        Heapsort          Runtime      Book Implementation      Class Implementation                  Python Implementation                      Quicksort          Runtime      Steps      Pivot      Partition Algorithm      Python Implementation        Bin/Bucket SortThe Sorting ProblemInput: A sequence of $n$ numbers $\\langle{a_1,a_2,…,a_n}\\rangle$Output: A permutation of the sequence in increasing or decreasing order.  Ascending          $a_i \\le a_{i+1}$ for all $1\\le i&lt;n$        Descending          $a_i \\ge a_{i+1}$ for all $1\\le i&lt;n$      Bubble SortCompares each pair of elements in an array and swaps them if they are out of order until the entire array is sorted.Running TimeBest Case: $\\Theta(n)$Worst Case: $\\Theta(n^2)$Loop Invariant  Enter the for loop on i with value j, array a[0...k-1] contains the k smallest elements of the set in sorted order.Python Implementationdef bubble_sort(a):    n = len(a)    for i in range(n-1):        for j in range(i+1, n):            if a[i] &gt; a[j]:                a[i], a[j] = a[j], a[i]    return aInsertion Sort  Builds a final sorted array one element at a time.  Iterates through an input array and removes one element per iteration  Finds the place the element belongs and places within the array.Running TimeBest Case: $\\Theta(n)$  Numbers are already sortedWorst Case: $\\Theta(n^2)$Python Implementationdef insertion_sort(array):    for slot in range(1, len(array)):         value = array[slot]        test_slot = slot - 1        while test_slot &gt; -1 and array[test_slot] &gt; value:            array[test_slot + 1] = array[test_slot]            test_slot = test_slot - 1        array[test_slot + 1] = value    return arrayMerge Sort  Recursive  Merges two pre-sorted arrays so that the resulting array is sorted.  Merges with an outside merge function.Recurrence$T(n)=2T(n/2) + O(n)$Running Time$\\Theta(nlgn)$Python Implementationdef merge_sort(array):    if len(array) &lt;= 1:        return array    else:        m = floor(len(array) / 2)   return merge(array[0:m], merge_sort(array[m:]))Selection Sort  Class uses abstractSort(a)Python ImplementationabstractSort(array):\tn, na = len(array), []\tfor i in range(n):\t\tsmallest = extraSmallestAndDelete(array)\t\tna.append(smallest)\treturn naHeapsort  A comparison based algorithm  Sorts in place  Uses a binary heap data structureRuntime$\\Theta(nlogn)$Book ImplementationClass Implementation  Uses a min-heapPython Implementationdef newheap(n):    return [0]*(n+1)def insert(a, e):    a[0] = a[0] + 1    a[a[0]] = e    heapfixup(a,a[0])def heapfixup(a,i):    while i &gt; 1:        parent = floor(i/2)        if a[p] &gt; a[i]:            a[p], a[i] = a[i], a[p]            i = p        else:            returndef heapsort(x):    n = len(x)    a = newheap(n)    for i in range(n):        insert(a, x[i])    for i in range(n):        x[i] = extractsmallest(a)    return xQuicksort  Picks a pivot and then sorts to the left of the pivot and then right.  Comparison-based  Uses divide-and-conquer          Recursive      RuntimeRecurrence: $T(n)=2T(\\frac{n}{2}+n)$Best Case: $O(nlogn)$Worst Case: $O(n^2)$  Occurs when array is already sortedAverage Case: $O(nlogn)$Steps  If the list is empty, return the list and terminate. (base case)  Choose a pivot element in the list.  Take all of the elements that are less than or equal to the pivot and use quicksort on them.  Take all of the elements that are greater than the pivot and use quicksort on them.  Return the concatenation of the quicksorted list of elements that are less than or equal to the pivot, the pivot, and the quicksorted list of elements that are greater than the pivot.Pivot  Select a random pivot.  Select the leftmost or rightmost element as the pivot.  Take the first, middle, and last value of the array, and choose the median of those three numbers as the pivot (Median of Three method)  Use a median finding algorithm such as the median-of-medians algorithm.Partition Algorithma - array to be sortedl - lower boundu - upper bounddef partition(a, l, u):    t = a[l]    m = l    for i in range(l+1, u+1):        if a[i] &lt; t:            m = m + 1            a[i] = a[m]            a[m] = a[i]        a[m] = a[l]        a[l] = a[m]    return mPython Implementationdef quicksort(a, l=0, u=None):    if u is None:        u = len(a) - 1    if l &lt; u:        m = partition(a, l, u)        quicksort(a, l, m-1)        quicksort(a, m+1, u)    return aBin/Bucket Sort  Not based on comparisons  Use knowledge of the data  Not in-place sorting",
        "url": "/notes/sorting-algorithms/"
      }
      
    
  };
</script>
<script src="/CSI-5007/js/lunr.min.js"></script>
<script src="/CSI-5007/js/search.js"></script>
</div>
        </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/CSI-5007/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Algorithms</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Algorithms</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/josh-corneille"><svg class="svg-icon"><use xlink:href="/CSI-5007/assets/minima-social-icons.svg#github"></use></svg> <span class="username">josh-corneille</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>CSI 5007 Notes</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>